{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf2936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 3 tasks.\n",
      "Output saved to: converted_jsons/project-7-at-2025-12-15-15-15-f68c6772_converted.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def extract_ls_data(input_file_path, output_file_path):\n",
    "    try:\n",
    "        # 1. Read the input JSON file\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            input_data = json.load(f)\n",
    "        \n",
    "        final_output = {}\n",
    "\n",
    "        # 2. Process each task in the Label Studio export\n",
    "        for task in input_data:\n",
    "            # Extract image filename from the 'data' section\n",
    "            # Format usually looks like: \"/data/local-files/?d=folder/images/filename.jpg\"\n",
    "            image_path_raw = task.get('data', {}).get('ocr', '') or task.get('data', {}).get('images', '')\n",
    "            \n",
    "            # Logic to clean the filename\n",
    "            if 'd=' in image_path_raw:\n",
    "                # Extract path after 'd=' and decode URI characters (e.g., %20)\n",
    "                clean_path = unquote(image_path_raw.split('d=')[1])\n",
    "                image_name = os.path.basename(clean_path)\n",
    "            else:\n",
    "                image_name = os.path.basename(image_path_raw)\n",
    "\n",
    "            # Prepare list for this image\n",
    "            img_entries = []\n",
    "\n",
    "            # Loop through annotations (usually one per task, but handled as a list)\n",
    "            for annotation in task.get('annotations', []):\n",
    "                results = annotation.get('result', [])\n",
    "                \n",
    "                # Dictionary to group separate components (poly, text, labels) by their shared UUID\n",
    "                grouped_items = {}\n",
    "\n",
    "                for item in results:\n",
    "                    # The 'id' field links the polygon to the transcription\n",
    "                    item_id = item.get('id')\n",
    "                    if not item_id:\n",
    "                        continue\n",
    "\n",
    "                    if item_id not in grouped_items:\n",
    "                        grouped_items[item_id] = {\n",
    "                            \"coordinates\": [],\n",
    "                            \"text\": \"\",\n",
    "                            \"labels\": [],\n",
    "                            \"original_width\": item.get('original_width'),\n",
    "                            \"original_height\": item.get('original_height')\n",
    "                        }\n",
    "\n",
    "                    value = item.get('value', {})\n",
    "                    item_type = item.get('type')\n",
    "\n",
    "                    # Extract Coordinates\n",
    "                    if item_type == 'polygon':\n",
    "                        grouped_items[item_id]['coordinates'] = value.get('points', [])\n",
    "                    \n",
    "                    # Extract Text\n",
    "                    elif item_type == 'textarea':\n",
    "                        # Label Studio stores text as a list [\"Text here\"]\n",
    "                        text_list = value.get('text', [])\n",
    "                        grouped_items[item_id]['text'] = \" \".join(text_list) if text_list else \"\"\n",
    "                    \n",
    "                    # Extract Labels (if needed for additional_data)\n",
    "                    elif item_type == 'labels':\n",
    "                        grouped_items[item_id]['labels'] = value.get('labels', [])\n",
    "\n",
    "                # Format the grouped items into the final list structure\n",
    "                for unique_id, data in grouped_items.items():\n",
    "                    # Only add if we actually have coordinates\n",
    "                    if data['coordinates']:\n",
    "                        entry = {\n",
    "                            \"coordinates\": data['coordinates'],\n",
    "                            \"text\": data['text'],\n",
    "                            \"additional_data\": {\n",
    "                                \"id\": unique_id,\n",
    "                                \"labels\": data['labels'],\n",
    "                                \"width\": data['original_width'],\n",
    "                                \"height\": data['original_height']\n",
    "                            }\n",
    "                        }\n",
    "                        img_entries.append(entry)\n",
    "\n",
    "            # Add to main dictionary\n",
    "            final_output[image_name] = img_entries\n",
    "\n",
    "        # 3. Write to the output JSON file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_output, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"Successfully processed {len(input_data)} tasks.\")\n",
    "        print(f\"Output saved to: {output_file_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Change these filenames as needed\n",
    "    INPUT_FILE = 'label-studio_exports/project-7-at-2025-12-15-15-15-f68c6772.json'\n",
    "    OUTPUT_FILE = 'converted_jsons/project-7-at-2025-12-15-15-15-f68c6772_converted.json'\n",
    "    \n",
    "    extract_ls_data(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41743874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: IMG-20251127-WA0004.jpg (24 boxes)\n",
      "Processing: IMG-20251127-WA0003.jpg (19 boxes)\n",
      "Processing: 019_2.jpg (19 boxes)\n",
      "\n",
      "Done! Check the 'annotated_images' directory.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "JSON_FILE = 'converted_jsons/project-7-at-2025-12-15-15-15-f68c6772_converted.json'        # The file created in the previous step\n",
    "IMAGE_FOLDER = 'images'          # Folder where your raw .jpg files are located\n",
    "OUTPUT_FOLDER = 'annotated_images' # Folder where results will be saved\n",
    "# ---------------------\n",
    "\n",
    "def draw_boxes_on_images():\n",
    "    # 1. Create output folder if it doesn't exist\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "    # 2. Load the JSON data\n",
    "    try:\n",
    "        with open(JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {JSON_FILE}\")\n",
    "        return\n",
    "\n",
    "    # 3. Iterate through each image in the JSON\n",
    "    for filename, annotations in data.items():\n",
    "        image_path = os.path.join(IMAGE_FOLDER, filename)\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image file not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        # We read as unchanged to preserve channels, but typically imread defaults to BGR\n",
    "        img = cv2.imread(image_path)\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"Error: Could not read image {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Get actual image dimensions to convert percentages to pixels\n",
    "        height, width = img.shape[:2]\n",
    "\n",
    "        print(f\"Processing: {filename} ({len(annotations)} boxes)\")\n",
    "\n",
    "        # 4. Iterate through each box/polygon in the list\n",
    "        for item in annotations:\n",
    "            points = item.get('coordinates', [])\n",
    "            \n",
    "            if not points:\n",
    "                continue\n",
    "\n",
    "            # Convert Label Studio percentage coordinates (0-100) to Pixel coordinates\n",
    "            # Format in JSON is usually [[x1, y1], [x2, y2], ...]\n",
    "            pixel_points = []\n",
    "            for pt in points:\n",
    "                x_pct, y_pct = pt[0], pt[1]\n",
    "                \n",
    "                x_px = int((x_pct / 100.0) * width)\n",
    "                y_px = int((y_pct / 100.0) * height)\n",
    "                \n",
    "                pixel_points.append([x_px, y_px])\n",
    "\n",
    "            # Reshape for OpenCV (requires a specific numpy array shape)\n",
    "            pts = np.array(pixel_points, np.int32)\n",
    "            pts = pts.reshape((-1, 1, 2))\n",
    "\n",
    "            # Draw the Polygon\n",
    "            # cv2.polylines(image, [pts], isClosed, color(BGR), thickness)\n",
    "            # Color: (0, 255, 0) is Green\n",
    "            cv2.polylines(img, [pts], True, (255, 0, 0), 1)\n",
    "            \n",
    "            # Optional: Draw the text ID or start of text slightly above the box\n",
    "            # extracted_text = item.get('text', '')[:15] # First 15 chars\n",
    "            # if extracted_text:\n",
    "            #     cv2.putText(img, extracted_text, (pixel_points[0][0], pixel_points[0][1] - 5), \n",
    "            #                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "        # 5. Save the annotated image\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "        cv2.imwrite(output_path, img)\n",
    "\n",
    "    print(f\"\\nDone! Check the '{OUTPUT_FOLDER}' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    draw_boxes_on_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1fc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n",
      "Images saved to: dataset/train\\images\n",
      "Metadata saved to: dataset/train\\metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eshan\\AppData\\Local\\Temp\\ipykernel_2900\\525437771.py:133: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  box = np.int0(box)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_JSON = 'converted_jsons/project-12-at-2025-12-15-14-48-2d2ece96_converted.json'\n",
    "INPUT_IMAGE_DIR = 'images'\n",
    "BASE_OUTPUT_DIR = 'dataset/train'\n",
    "OUTPUT_IMAGE_DIR = os.path.join(BASE_OUTPUT_DIR, 'images')\n",
    "METADATA_FILE = os.path.join(BASE_OUTPUT_DIR, 'metadata.csv')\n",
    "# ---------------------\n",
    "\n",
    "def order_points(pts):\n",
    "    \"\"\"\n",
    "    Orders coordinates in the form: top-left, top-right, bottom-right, bottom-left.\n",
    "    Essential for perspective warping.\n",
    "    \"\"\"\n",
    "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "\n",
    "    # The top-left point will have the smallest sum, whereas\n",
    "    # the bottom-right point will have the largest sum\n",
    "    s = pts.sum(axis=1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "\n",
    "    # The top-right point will have the smallest difference,\n",
    "    # whereas the bottom-left point will have the largest difference\n",
    "    diff = np.diff(pts, axis=1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "\n",
    "    return rect\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "    \"\"\"\n",
    "    Applies perspective transform to obtain a top-down, \"straightened\" view of the image.\n",
    "    \"\"\"\n",
    "    # 1. Obtain a consistent order of the points\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "\n",
    "    # 2. Compute the width of the new image\n",
    "    # Maximum distance between bottom-right and bottom-left or top-right and top-left\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    # 3. Compute the height of the new image\n",
    "    # Maximum distance between top-right and bottom-right or top-left and bottom-left\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "\n",
    "    # 4. Construct the set of destination points to obtain a \"birds eye view\",\n",
    "    # (i.e. top-down view) of the image, specifying points in the TL, TR, BR, BL order\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "        [0, maxHeight - 1]], dtype=\"float32\")\n",
    "\n",
    "    # 5. Compute the Perspective Transform Matrix and then apply it\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "\n",
    "    return warped\n",
    "\n",
    "def process_dataset():\n",
    "    # Create directories\n",
    "    if not os.path.exists(OUTPUT_IMAGE_DIR):\n",
    "        os.makedirs(OUTPUT_IMAGE_DIR)\n",
    "\n",
    "    # Load JSON\n",
    "    try:\n",
    "        with open(INPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {INPUT_JSON} not found.\")\n",
    "        return\n",
    "\n",
    "    # Open CSV for writing\n",
    "    with open(METADATA_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['file_name', 'text']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Iterate over images\n",
    "        for filename, annotations in data.items():\n",
    "            img_path = os.path.join(INPUT_IMAGE_DIR, filename)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Skipping missing image: {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Load the original image\n",
    "            original_img = cv2.imread(img_path)\n",
    "            if original_img is None:\n",
    "                continue\n",
    "\n",
    "            h, w = original_img.shape[:2]\n",
    "            \n",
    "            # Remove extension from filename for crop naming\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "            for idx, item in enumerate(annotations):\n",
    "                points = item.get('coordinates', [])\n",
    "                text_content = item.get('text', '').strip()\n",
    "\n",
    "                # Skip if no coordinates or empty text (optional: remove 'not text_content' if you want empty text files)\n",
    "                if not points: \n",
    "                    continue\n",
    "\n",
    "                # 1. Convert Percentage Coordinates to Pixels\n",
    "                pixel_points = []\n",
    "                for pt in points:\n",
    "                    px = int((pt[0] / 100.0) * w)\n",
    "                    py = int((pt[1] / 100.0) * h)\n",
    "                    pixel_points.append([px, py])\n",
    "                \n",
    "                np_points = np.array(pixel_points, dtype=\"float32\")\n",
    "\n",
    "                # 2. Logic: Handle different polygon shapes\n",
    "                # Perspective warp requires exactly 4 points.\n",
    "                # If the user drew a complex polygon (5+ points) or a triangle (3 points),\n",
    "                # we calculate the \"Minimum Area Rotated Rectangle\" that fits those points,\n",
    "                # get the 4 corners of *that* rectangle, and warp that.\n",
    "                \n",
    "                rect = cv2.minAreaRect(np_points)\n",
    "                box = cv2.boxPoints(rect)\n",
    "                box = np.int0(box)\n",
    "                \n",
    "                # Perform the perspective warp (crop & straighten)\n",
    "                try:\n",
    "                    cropped_img = four_point_transform(original_img, box.astype(\"float32\"))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cropping {filename} item {idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # 3. Save the Crop\n",
    "                crop_filename = f\"{base_name}_crop_{idx}.jpg\"\n",
    "                save_path = os.path.join(OUTPUT_IMAGE_DIR, crop_filename)\n",
    "                \n",
    "                cv2.imwrite(save_path, cropped_img)\n",
    "\n",
    "                # 4. Write to CSV\n",
    "                # Requirement: file_name should be ./image/{image_crop_name}\n",
    "                relative_path = f\"./image/{crop_filename}\"\n",
    "                \n",
    "                writer.writerow({\n",
    "                    'file_name': relative_path, \n",
    "                    'text': text_content\n",
    "                })\n",
    "\n",
    "    print(f\"Processing complete.\")\n",
    "    print(f\"Images saved to: {OUTPUT_IMAGE_DIR}\")\n",
    "    print(f\"Metadata saved to: {METADATA_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
